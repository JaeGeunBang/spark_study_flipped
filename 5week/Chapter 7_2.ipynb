{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc = org.apache.spark.sql.SparkSession@7ba4c2f9\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.4.3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val sc = SparkSession.builder().getOrCreate()\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [InvoiceNo: string, StockCode: string ... 6 more fields]\n",
       "dfWithDate = [InvoiceNo: string, StockCode: string ... 7 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[InvoiceNo: string, StockCode: string ... 7 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{to_date, col}\n",
    "val df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"./all/*.csv\").coalesce(5)\n",
    "val dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfNoNull = [InvoiceNo: string, StockCode: string ... 7 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[InvoiceNo: string, StockCode: string ... 7 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfNoNull = dfWithDate.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 롤업\n",
    "다양한 컬럼을 그룹화 키로 설정된 조합뿐 아니라 데이터셋에서 볼 수 있는 실제 조합을 모두 살펴볼 수 있음. <br>\n",
    "null 값을 가진 로우에서 전체 날짜의 합계를 확인할 수 있습니다. 롤업된 두 개의 칼럼값이 모두 null인 로우는 두 컬럼에 속한 레코드의 전체 합계를 나타냄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.sum\n",
    "val rolledUpDf = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\n",
    "    .selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\").orderBy(\"Date\")\n",
    "rolledUpDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 큐브\n",
    "롤업을 고차원적으로 사용할 수 있게 함. 큐브는 요소들을 계층적으로 다루는 대신 모든 차원에 대해 동일한 작업을 수행함. <br>\n",
    "- 전체 날짜와 모든 국가에 대한 합계\n",
    "- 모든 국가의 날짜별 합계\n",
    "- 날짜별 국가별 합계\n",
    "- 전체 날짜의 국가별 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rolledUpDf = dfNoNull.cube(\"Date\", \"Country\").agg(sum(\"Quantity\"))\n",
    "    .selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\").orderBy(\"Date\")\n",
    "rolledUpDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3 그룹화 메타데이터\n",
    "큐브와 롤업을 사용하다 보면 집계 수준에 따라 쉽게 필터링하기 위해 집계 수준을 조회하는 경우가 발생함. <br>\n",
    "grouping_id를 사용해 집계 수준을 명시하는 컬럼을 제공함\n",
    "- 0: customerId, stockCode별 조합에 따라 총 수량 제공\n",
    "- 1: 구매한 물품에 관계 없이 customerId를 기반으로 총 수량 제공\n",
    "- 2: 개별 재고 코드의 모든 집계 결과를 나타냄\n",
    "- 3: 가장 높은 계층의 집계 결과를 나타냄. customerId나 stockCode에 관계 없이 총 수량을 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:26: error: not found: value dfNoNull\n",
       "       val rolledUpDf = dfNoNull.cube(\"customerId\", \"stockCode\").agg(grouping_id(), sum(\"Quantity\"))\n",
       "                        ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{grouping_id, sum, expr, col}\n",
    "val rolledUpDf = dfNoNull.cube(\"customerId\", \"stockCode\").agg(grouping_id(), sum(\"Quantity\"))\n",
    "    .orderBy(col(\"grouping_id()\").desc)\n",
    "rolledUpDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.4 피벗\n",
    "로우를 컬럼으로 변환할 수 있음. 현재 데이터셋에서는 Country 컬럼이 있음.  <br>\n",
    "피벗을 사용해 국가별로 집계 함수를 적용할 수 있으며 쿼리를 사용해 쉽게 결과를 확인할 수 있음.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:25: error: not found: value dfWithDate\n",
       "       val pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()\n",
       "                     ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 사용자 정의 집계 함수\n",
    "직접 제작한 함수, 비즈니스 규칙에 기반을 둔 자체 집계 함수를 정의하는 방법 <br>\n",
    "UDAF를 사용해 입력 데이터 그룹에 직접 개발한 연산을 수행할 수 있다. <br> \n",
    "중간 결과를 단일 AggregationBuffer에 저장해 관리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expression.MutableAggregationBuffer\n",
    "import org.apache.spark.sql.expression.UserDefinedAggregateFunction\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "class BoolAnd extends UserDefinedAggregateFunction {\n",
    "    def inputSchema: org.apache.spark.sql.types.StructType =\n",
    "        StructType(StructField(\"value\", BooleanType) :: Nil)\n",
    "    def bufferSchema: StrucType = StrucType(StrucField(\"result\", BooleanType) :: Nil)\n",
    "    def dataType: DataType = BooleanType\n",
    "    def deterministric: Boolean = true\n",
    "    def initialize(buffer: MutableAggregationBuffer): Unit = {\n",
    "        buffer(0) = true\n",
    "    }\n",
    "    \n",
    "    def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n",
    "        buffer(0) = buffer.getAs[Boolean](0) && input.getAs[Boolean](0)\n",
    "    }\n",
    "    def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n",
    "        buffer1(0) = buffer1.getAs[Boolean](0) && buffer2.getAs[Boolean](0)\n",
    "    }\n",
    "    def evaluate(buffer: Row): Any = {\n",
    "        buffer(0)\n",
    "    }\n",
    "}\n",
    "\n",
    "val ba = new BoolAnd\n",
    "spark.udf.register(\"boolean\", ba)\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "spark.range(1)\n",
    "    .selectExpr(\"explode(array(TRUE, TRUE, TRUE)) as t\")\n",
    "    .selectExpr(\"explode(array(TRUE, FALSE, TRUE)) as f\", \"t\")\n",
    "    .select(ba(col(\"t\")), expr(\"booland(f)\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
