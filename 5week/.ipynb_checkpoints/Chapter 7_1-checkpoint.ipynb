{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc = org.apache.spark.sql.SparkSession@3c6166b2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.4.3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val sc = SparkSession.builder().getOrCreate()\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 집계 연산\n",
    "\n",
    "집계를 수행하려면 **키, 그룹**을 지정하고 하나 이상의 컬럼을 변환하는 방법을 지정하는 **집계 함수**를 사용함 <br> \n",
    "집계 함수는 여러 입력 값이 주어지면 그룹별로 결과를 생성함.<br> \n",
    "<br> \n",
    "다음과 같은 그룹화 데이터 타입을 생성할 수 있다. <br> \n",
    "- 가장 간단한 형태의 그룹화는 select 구문에서 집계를 수행해 Dataframe의 전체 데이터를 요약함\n",
    "- group by는 하나 이상의 키를 지정하며, 값을 가진 칼럼을 변환하기 위해 다른 집계 함수를 사용할 수 있음\n",
    "- window 는 하나 이상의 키를 지정하며, 값을 가진 칼럼을 변환하기 위해 사른 집계 함수를 사용할 수 있음. 하지만 함수의 입력으로 사용할 로우는 현재 로우와 어느정도 연관성이 있어야 함(?)\n",
    "- grouping set 은 서로 다른 레벨의 값을 집계할 때 사용함. SQL, DataFrame의 롤업, 큐브를 사용할 수 있음\n",
    "- rollup은 하나 이상의 키를 지정하며, 컬럼을 변환하는데 다른 집계 함수를 사용하여 계층적으로 요약된 값을 구함\n",
    "- cube는 하나 이상의 키를 지정하며, 컬럼을 변환하는데 다른 집계 함수를 사용할 수 있고, 모든 컬럼 조합에 대한 요약된 값을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |12/1/2010 8:26|2.75     |17850     |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[InvoiceNo: string, StockCode: string ... 6 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 데이터 읽음\n",
    "// 적은 수로 분할할 수 있도록 리파티셔닝 진행하며 빠르게 접근할 수 있도록 캐싱.\n",
    "val df = sc.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(\"./all/*.csv\").coalesce(5)\n",
    "df.unpersist()\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "df.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 간단한 집계\n",
    "// count()는 action 이기 때문에 바로 결과를 반환함.\n",
    "df.count() == 541909"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 집계 함수\n",
    "집계 함수는 org.apache.spark.sql.functions 패키지에서 찾아볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// count\n",
    "import org.apache.spark.sql.functions.count\n",
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// countDistinct\n",
    "import org.apache.spark.sql.functions.countDistinct\n",
    "df.select(countDistinct(\"StockCode\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// approx_count_distinct: 근사치 count로 최대 추정 오류율이라는 파라미터를 사용함.\n",
    "// 근사치를 계산하지만, countDistinct 함수보다 빠르게 결과를 반환함\n",
    "// 대규모 데이터 셋을 사용할 수록 더 빠른 성능을 보임.\n",
    "import org.apache.spark.sql.functions.approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// first, last\n",
    "import org.apache.spark.sql.functions.{first, last}\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// min, max\n",
    "import org.apache.spark.sql.functions.{min, max}\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//sum\n",
    "import org.apache.spark.sql.functions.{sum}\n",
    "df.select(sum(\"Quantity\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// sumDinsinct: 고유값을 합산함\n",
    "import org.apache.spark.sql.functions.{sumDistinct}\n",
    "df.select(sumDistinct(\"Quantity\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[sum(cast(Quantity#13 as bigint)), count(Quantity#13), avg(cast(Quantity#13 as bigint))])\n",
      "+- Exchange SinglePartition\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_sum(cast(Quantity#13 as bigint)), partial_count(Quantity#13), partial_avg(cast(Quantity#13 as bigint))])\n",
      "      +- InMemoryTableScan [Quantity#13]\n",
      "            +- InMemoryRelation [InvoiceNo#10, StockCode#11, Description#12, Quantity#13, InvoiceDate#14, UnitPrice#15, CustomerID#16, Country#17], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                  +- Coalesce 5\n",
      "                     +- *(1) FileScan csv [InvoiceNo#10,StockCode#11,Description#12,Quantity#13,InvoiceDate#14,UnitPrice#15,CustomerID#16,Country#17] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/spark_study_flipped/5week/all/online-retail-dataset.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:string,StockCode:string,Description:string,Quantity:int,InvoiceDate:string,UnitP...\n"
     ]
    }
   ],
   "source": [
    "// avg\n",
    "import org.apache.spark.sql.functions.{avg, sum, count, expr}\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\")\n",
    ")\n",
    ".selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\"\n",
    ")\n",
    ".show()\n",
    "//.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[var_pop(cast(Quantity#13 as double)), var_samp(cast(Quantity#13 as double)), stddev_pop(cast(Quantity#13 as double)), stddev_samp(cast(Quantity#13 as double))])\n",
      "+- Exchange SinglePartition\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_var_pop(cast(Quantity#13 as double)), partial_var_samp(cast(Quantity#13 as double)), partial_stddev_pop(cast(Quantity#13 as double)), partial_stddev_samp(cast(Quantity#13 as double))])\n",
      "      +- InMemoryTableScan [Quantity#13]\n",
      "            +- InMemoryRelation [InvoiceNo#10, StockCode#11, Description#12, Quantity#13, InvoiceDate#14, UnitPrice#15, CustomerID#16, Country#17], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                  +- Coalesce 5\n",
      "                     +- *(1) FileScan csv [InvoiceNo#10,StockCode#11,Description#12,Quantity#13,InvoiceDate#14,UnitPrice#15,CustomerID#16,Country#17] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/spark_study_flipped/5week/all/online-retail-dataset.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:string,StockCode:string,Description:string,Quantity:int,InvoiceDate:string,UnitP...\n"
     ]
    }
   ],
   "source": [
    "// 분산, 표준편차\n",
    "// 분산과 표준편차는 데이터가 분포된 정도를 측정하는 방법.\n",
    "// 표본표준편차(samp), 모표준편차(pop) 방식을 지원함.\n",
    "import org.apache.spark.sql.functions.{var_pop, stddev_pop}\n",
    "import org.apache.spark.sql.functions.{var_samp, stddev_samp}\n",
    "\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n",
    "         stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\"))\n",
    ".show()\n",
    "//.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|  skewness(Quantity)|kurtosis(Quantity)|\n",
      "+--------------------+------------------+\n",
      "|-0.26407557610527843|119768.05495536518|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// 비대칭도와 첨도\n",
    "// 데이터의 변곡점을 측정하는 방법 (확률변수, 확률분포로 데이터를 모델링할 때 중요함)\n",
    "// 비대칭도: 데이터 평균의 비대칭 정도를 측정\n",
    "// 첨도: 데이터 끝 부분을 측정함.\n",
    "import org.apache.spark.sql.functions.{skewness, kurtosis}\n",
    "\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|     4.912186085637639E-4|            1052.7260778752732|             1052.7280543913773|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//공분산, 상관관계\n",
    "// cov, corr를 통해 공분산, 상관관계를 계산할 수 있음.\n",
    "// 공분산: 데이터 입력 값에 따라 다른 범위를 가짐 (얜 표본공분산, 모공분산을 계산할 수 있음)\n",
    "// 상관관계: 피어슨 상관계수를 측정하여 -1,1 사이의 값을 가짐.\n",
    "import org.apache.spark.sql.functions.{corr, covar_pop, covar_samp}\n",
    "\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_pop(\"InvoiceNo\", \"Quantity\"),\n",
    "         covar_samp(\"InvoiceNo\", \"Quantity\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 복합 데이터 타입의 집계\n",
    "// 복합 데이터 타입을 사용해 집계를 수행할 수 있음. 예를 들어 특정 칼럼의 값을 리스트로 수집하거나 셋 데이터 타입으로 고윳값만 수집할 수 있음.\n",
    "import org.apache.spark.sql.functions.{collect_set, collect_list}\n",
    "df.agg(collect_set(\"Country\"),collect_list(\"Country\"))\n",
    "//.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 그룹화\n",
    "\n",
    "Dataframe 수준의 집계 뿐만 아니라, 데이터 그룹 기반의 집계를 수행하는 경우가 더 많음. <br>\n",
    "데이터 그룹 기반의 집계는 단일 컬럼의 데이터를 그룹화하고 해당 그룹의 다른 여러 컬럼을 사용해 계산하기 위해 카테고리형 데이터를 사용함. <br>\n",
    "그룹의 기준이 되는 컬럼을 여러 개 지정할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|536846   |14573     |76   |\n",
      "|537026   |12395     |12   |\n",
      "|537883   |14437     |5    |\n",
      "|538068   |17978     |12   |\n",
      "|538279   |14952     |7    |\n",
      "+---------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// 그룹화\n",
    "// 집계를 하나만 쓸땐, 바로 집계함수를 사용 (count())\n",
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------------------+---------------+\n",
      "|InvoiceNo|quan|avg               |count(Quantity)|\n",
      "+---------+----+------------------+---------------+\n",
      "|536596   |6   |1.5               |6              |\n",
      "|536938   |14  |33.142857142857146|14             |\n",
      "|537252   |1   |31.0              |1              |\n",
      "|537691   |20  |8.15              |20             |\n",
      "|538041   |1   |30.0              |1              |\n",
      "+---------+----+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// 표현식을 이용한 그룹화\n",
    "// 집계를 여러 사용하고 싶을 떄, agg(집계 함수1, 집계 함수2, 집계 함수3)\n",
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    count(\"Quantity\").alias(\"quan\"),\n",
    "    avg(\"Quantity\").alias(\"avg\"),\n",
    "    expr(\"count(Quantity)\")\n",
    ").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536596|               1.5|  1.1180339887498947|\n",
      "|   536938|33.142857142857146|  20.698023172885524|\n",
      "|   537252|              31.0|                 0.0|\n",
      "|   537691|              8.15|   5.597097462078001|\n",
      "|   538041|              30.0|                 0.0|\n",
      "|   538184|12.076923076923077|   8.142590198943392|\n",
      "|   538517|3.0377358490566038|  2.3946659604837897|\n",
      "|   538879|21.157894736842106|  11.811070444356483|\n",
      "|   539275|              26.0|  12.806248474865697|\n",
      "|   539630|20.333333333333332|  10.225241100118645|\n",
      "|   540499|              3.75|  2.6653642652865788|\n",
      "|   540540|2.1363636363636362|  1.0572457590557278|\n",
      "|  C540850|              -1.0|                 0.0|\n",
      "|   540976|10.520833333333334|   6.496760677872902|\n",
      "|   541432|             12.25|  10.825317547305483|\n",
      "|   541518| 23.10891089108911|  20.550782784878713|\n",
      "|   541783|11.314285714285715|   8.467657556242811|\n",
      "|   542026| 7.666666666666667|   4.853406592853679|\n",
      "|   542375|               8.0|  3.4641016151377544|\n",
      "|  C542604|              -8.0|  15.173990905493518|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// 맵을 이용한 그룹화\n",
    "df.groupBy(\"InvoiceNo\").agg(\"Quantity\"->\"avg\", \"Quantity\"->\"stddev_pop\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 윈도우 함수\n",
    "\n",
    "특정 **윈도우** 를 대상으로 집계 연산을 수행함. <br>\n",
    "\n",
    "group-by와 차이점?\n",
    "- group-by를 사용하면 **Dataframe**에 모든 로우 레코드에 대한 결괎값을 계산함.\n",
    "- 허나, 윈도우 함수는 **프레임**에 입력되는 모든 로우에 대한 결괎값을 계산함. (프레임: 로우 그룹 기반의 테이블을 뜻하며, 각 로우는 하나 이상의 프레임에 할당될 수 있음)\n",
    "\n",
    "세가지 종류의 운도우 함수를 지원함\n",
    "- 랭크 함수\n",
    "- 분석 함수\n",
    "- 집계 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |date      |\n",
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|2010-12-01|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|2010-12-01|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |12/1/2010 8:26|2.75     |17850     |United Kingdom|2010-12-01|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|2010-12-01|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|2010-12-01|\n",
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfWithDate = [InvoiceNo: string, StockCode: string ... 7 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[InvoiceNo: string, StockCode: string ... 7 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 윈도우함수를 위해 date 컬럼이 필요하다. (InvoceDate에서 날짜 정보만 추출)\n",
    "import org.apache.spark.sql.functions.{col, to_date}\n",
    "\n",
    "val dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")\n",
    "dfWithDate.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|date      |Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|12346     |2011-01-18|74215   |1           |1                |74215              |\n",
      "|12346     |2011-01-18|-74215  |2           |2                |74215              |\n",
      "|12347     |2010-12-07|36      |1           |1                |36                 |\n",
      "|12347     |2010-12-07|30      |2           |2                |36                 |\n",
      "|12347     |2010-12-07|24      |3           |3                |36                 |\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "windowSpec = org.apache.spark.sql.expressions.WindowSpec@14462fec\n",
       "maxPurchaseQuantity = max(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n",
       "purchaseDenseRank = DENSE_RANK() OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n",
       "purchaseRank = RANK() OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RANK() OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "// 윈도우 명세 생성\n",
    "// partitionBy()는 파티셔닝 스키마의 개념이 아닌, 그룹을 어떻게 나눌지 결정함 (프레임 만드는 과정)\n",
    "// orderBy()는 파티션 정렬 방식을 정의\n",
    "// rowsBetween() 입력된 로우의 참조를 기반으로 프레임에 로우가 포함될 수 있는지 결정함. 아래 예제는 첫 로우, 현재 로우까지 확인. (?)\n",
    "val windowSpec = Window.partitionBy(\"CustomerId\", \"date\").orderBy(col(\"Quantity\").desc)\n",
    "                    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "import org.apache.spark.sql.functions.{max, dense_rank, rank}\n",
    "\n",
    "val maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec) //시간대별 최대 구매 개수를 구하는 예제\n",
    "val purchaseDenseRank = dense_rank().over(windowSpec) // 모든 고객에 대해 최대 구매수량을 가진 날짜가 언제인지 구할 수 있음.\n",
    "val purchaseRank = rank().over(windowSpec) // rank를 사용하면 동일한 값이 나오거나 중복 로우가 발생할 수 있음(?)\n",
    "\n",
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\n",
    "    .select(\n",
    "        col(\"CustomerId\") ,\n",
    "        col(\"date\"),\n",
    "        col(\"Quantity\"),\n",
    "        purchaseRank.alias(\"quantityRank\") ,\n",
    "        purchaseDenseRank.alias(\"quantityDenseRank\") ,\n",
    "        maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")\n",
    "    ).show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfNoNull = dfWithData.drop()\n",
    "\n",
    "val rolledUpDF = dfNoNull.rollup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
