{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc = org.apache.spark.sql.SparkSession@72b079ac\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.4.3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val sc = SparkSession.builder().getOrCreate()\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 데이터 소스\n",
    "\n",
    "스파크에서 지원하는 핵심 데이터 소스\n",
    "- CSV\n",
    "- JSON\n",
    "- 파케이\n",
    "- ORC\n",
    "- JDBC / ODBC 연결\n",
    "- 일반 텍스트 파일\n",
    "\n",
    "또한 수많은 데이터 소스들을 지원함\n",
    "- 카산드라\n",
    "- HBASE\n",
    "- 몽고디비\n",
    "- AWS Redshift\n",
    "- XML\n",
    "- 기타 수많은 데이터 소스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1 읽기 API 구조\n",
    "\n",
    "```DataframReader.format(...).option(\"key\", \"value\").schema(...).load()```\n",
    "\n",
    "모든 데이터 소스를 읽을 때 위와 같은 형식을 사용함. <br>\n",
    "format 메서드는 선택적으로 사용할 수 있으며, 기본값은 파케이 <br>\n",
    "option 메서드를 사용해 데이터를 읽는 방법에 대한 파라미터를 키-값 쌍으로 설정할 수 있다. <br>\n",
    "schema 메서드는 데이터 소스에서 스키마를 제공하거나, 추론 기능을 사용하려는 경우 선택적으로 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2 데이터 읽기의 기초\n",
    "\n",
    "스파크 데이터를 읽을 때 기본적으로 DataFrameReader를 사용하며, 이는 SparkSession의 read 속성으로 접근할 수 있다.\n",
    "```spark.read ```\n",
    "\n",
    "이후 다음 값을 지정 해야함.\n",
    "- 포맷\n",
    "- 스키마\n",
    "- 읽기 모드\n",
    "- 옵션\n",
    "\n",
    "```\n",
    "spark.read.format(\"csv\")\n",
    "    .option(\"mode\", \"FAILFAST\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"path\", \"./../../\")\n",
    "    .schema(someSchame)\n",
    "    .load()\n",
    "```\n",
    "\n",
    "#### 읽기 모드\n",
    "읽기 모드는 아래와 같은 종류가 있다.\n",
    "- permissive: 오류 레코드의 모든 필드를 null로 설정하고, 모든 오류 레코드를 _corrupt_record라는 문자열 컬럼에 기록함.\n",
    "- dropMalformed: 형식에 맞지 않는 레코드가 포함된 로우를 제거함.\n",
    "- failFast: 형식에 맞지 않는 레코드를 만나면 즉시 종료함.\n",
    "\n",
    "기본 값은 permissive 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.3  쓰기 API 구조\n",
    "```DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBY(...).save()```\n",
    "\n",
    "format 메서드는 선택적으로 사용할 수 있으며 기본값은 파케이 포맷.\n",
    "option 메서드를 사용해 데이터 쓰기 방법을 설정함.\n",
    "partitionBy, bucketBy, sortBY 메서드는 파일 기반의 데이터 소스에서만 동작하여, 최종 파일 배치 형태를 제어할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.4 데이터 쓰기의 기초\n",
    "읽기와 매우 유사하며, DataFrameWriter를 사용함.\n",
    "```\n",
    "dataframe.write.format(\"csv\")\n",
    "    .option(\"mode\", \"OVERWRITE\")\n",
    "    .option(\"dataFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"path\", \"../../../\")\n",
    "    .save()\n",
    "```\n",
    "\n",
    "#### 저장 모드\n",
    "저장 모드는 아래와 같다.\n",
    "- append: 해당 경로에 이미 존재하는 파일목록에 결과 파일을 추가함.\n",
    "- overwrite: 이미 존재하는 모든 데이터를 완전히 덮어씀.\n",
    "- errorIfExists: 해당 경로에 데이터나 파일이 존재하는 경우 오류를 발생시키며 쓰기 작업이 실패함.\n",
    "- ignore: 해당 경로에 데이터나 파일이 존재하는 경우 아무런 처리도 하지 않음\n",
    "\n",
    "기본 값은 errorIfExists 이다. 포맷에 따라 저장 모드가 지원되는 경우도 있고 안되는 경우도 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 CSV 파일\n",
    "콤파(,)로 구분된 값을 의미함. 각 줄이 단일 레코드가 되며 레코드의 각 필드를 콤마(,)로 구분하는 텍스트 파일 포맷. <br>\n",
    "```\n",
    "id1,name1,description1\n",
    "id2,name2,description2\n",
    "id3,name3,description3\n",
    "```\n",
    "CSV는 실제 운영환경에서는 어떤 내용이 들어있는지, 어떠한 구조로 되어있는지 등 다양한 전제를 만들어낼 수 없다. 그렇기에 수많은 옵션들을 제공함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2 CSV 파일 읽기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "myManualSchema = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "\n",
    "val myManualSchema = new StructType(Array(\n",
    "    new StructField(\"DEST_COUNTRY_NAME\", StringType, true) ,\n",
    "    new StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true) ,\n",
    "    new StructField(\"count\", LongType, true)\n",
    "))\n",
    "\n",
    "sc.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"mode\", \"FAILFAST\")\n",
    "    .schema(myManualSchema)\n",
    "    .load(\"../data/flight-data/csv/2010-summary.csv\")\n",
    "    .show(5, false)\n",
    "// 정상동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "\n",
    "val myManualSchema = new StructType(Array(\n",
    "    new StructField(\"DEST_COUNTRY_NAME\", LongType, true) ,\n",
    "    new StructField(\"ORIGIN_COUNTRY_NAME\", LongType, true) ,\n",
    "    new StructField(\"count\", LongType, true)\n",
    "))\n",
    "\n",
    "sc.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"mode\", \"FAILFAST\")\n",
    "    .schema(myManualSchema)\n",
    "    .load(\"../data/flight-data/csv/2010-summary.csv\")\n",
    "    .show(5, false)\n",
    "// 비정상동작 \n",
    "\n",
    "// 실제 스키마와 일치 하지 않지만, 스파크는 어떠한 문제를 찾지 못함.\n",
    "//// 왜냐하면, 스파크가 실제로 데이터를 읽어들이는 시점에 문제가 발생함. 즉, 데이터가 지정된 스키마가 일치하지 않으므로 스키마 잡은 시작하자마자 종료됨.\n",
    "//// 또한, 스파크는 지연 연산 특성이 있으므로 DataFrame 정의 시점이 아닌 잡 실행 시점에만 오류가 발생함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.3 CSV 파일 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csvFile = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// CSV --> TSV\n",
    "val csvFile = sc.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\").option(\"mode\", \"FAILFAST\").schema(myManualSchema)\n",
    "    .load(\"../data/flight-data/csv/2010-summary.csv\")\n",
    "\n",
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").save(\"../tmp/my-tsv-file.tsv\")\n",
    "// 실제로 쓰는 시점에 DataFrame의 파티션 수를 반영함. 만약 사전에 데이터를 분할했다면, 파일 수가 달라질 수 있을 것.\n",
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").partitionBy(\"DEST_COUNTRY_NAME\").save(\"../tmp_5/my-tsv-file.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 JSON 파일\n",
    "자바스크립트에서 사용하는 파일 형식으로, 자바스크립트 객체 표기법. (JavaScript Object Notation) <br>\n",
    "스파크에서는 줄로 구분된 JSON을 기본적으로 사용함. 이런 방식은 큰 JSON 객체나 배열을 하나씩 가지고 있는 파일을 다루는 것보다 대조적인 부분(?)\n",
    "\n",
    "multiLine 옵션을 사용해 줄로 구분된 방식과, 여러 줄로 구성된 방식을 선택적으로 사용할 수 있다.<br>\n",
    "이 옵션을 true로 설정하면 전체 파일을 하나의 JSON 객체로 읽을 수 있다. <br>\n",
    "스파크는 JSON 파일을 파싱한 다음에 Dataframe을 생성한다. <br>\n",
    "\n",
    "줄로 구분된 JSON은\n",
    "- 전체 파일을 읽어 들인 다음 저장하는 방식이 아니기 때문에 새로운 레코드를 추가할 수 있다. 다른 포맷에 비해 안정적인 포맷이므로 이 방식을 사용하는 것이 좋다.\n",
    "- 구조화 되어있고, 최소한의 기본 데이터 타입이 존재함. 즉, 스파크는 적합한 데이터 타입을 추정할 수 있어 원할하게 처리할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2 JSON 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jsonFile = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsonFile = sc.read.format(\"json\").option(\"mode\", \"FAILFAST\").schema(myManualSchema)\n",
    "    .load(\"../data/flight-data/json/2010-summary.json\")\n",
    "jsonFile.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.3 JSON 파일 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"json\").mode(\"overwrite\").save(\"../tmp/my-json-file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 파케이 파일\n",
    "파케이는 다양한 스토리지 최적화 기술을 제공하는 오픈소스로 만들어진 컬럼 기반의 데이터 저장 방식. 특히 분석 워크로드에 최적화되어 있음. <br>\n",
    "저장소 공간을 절약할 수 있으며, 전체 파일을 읽는 대신 개별 컬럼을 읽을 수 있으며, 컬럼 기반의 압축 기능을 제공함. <br>\n",
    "아파치 스파크와 잘 호환되기 때문에 **스파크의 기본 파일 포맷**이기도 함. 읽기 연산시 JSON, CSV 보다 훨씬 효율적으로 동작하기 때문에 장기 저장용 데이터는 파케이 포맷으로 저장하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.1 파케이 파일 읽기\n",
    "\n",
    "옵션이 많이 없다. 자체 스키마를 사용해 데이터를 저장하기 때문이다. <br>\n",
    "파케이 파일은 스키마가 파일 자체에 내장되어 있기 때문에 따로 추정할 필요가 없다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.read.format(\"parquet\").load(\"../data/flight-data/parquet/2010-summary.parquet\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"parquet\").mode(\"overwrite\").save(\"../tmp/my-parquet-file.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 ORC 파일\n",
    "하둡 워크로드를 위해 설계된 자기 기술적이며 데이터 타입을 인식할 수 있는 컬럼 기반의 파일 포맷. <br>\n",
    "대규모 스트리밍 읽기에 최적화되어 있을 뿐 아니라 필요한 로우를 신속하게 찾아낼 수 있는 기능이 통합되어 있음. <br>\n",
    "파케이와 매우 유사하지만, 근본적인 차이점으로..\n",
    "- 파케이는 스파크에 최적화\n",
    "- ORC는 하이브에 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// ORC 파일 읽기, 쓰기\n",
    "sc.read.format(\"orc\").load(\"../data/flight-data/orc/2010-summary.orc\").show(5)\n",
    "csvFile.write.format(\"orc\").mode(\"overwrite\").save(\"../tmp/my-org-file.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6 SQL 데이터베이스\n",
    "다양한 SQL 시스템에 연결할 수 있다. (MySQL, postgreSQL, Oracle, SQLite 등)\n",
    "\n",
    "./bin/spark-shell \\ <br>\n",
    "--driver-class-path ..jar (클래스 패스 추가) <br>\n",
    "--jars ..jar (저장소에 파일 복사) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//  9.6.1 데이터 베이스 읽기\n",
    "val driver = \"org.splite.JDBC\"\n",
    "val path = \"/data/flight-data/jdbc/my-sqlite.db\"\n",
    "val url = s\"jdbc:splite:/${path}\"\n",
    "val tablename = \"flight_info\"\n",
    "\n",
    "import java.sql.DriverManager\n",
    "val connection = DriverManager.getConnection(url)\n",
    "// 스파크 드라이버가 데이터베이스에 접속할 수 있는지 확인할 수 있음.\n",
    "connection.isClosed()\n",
    "connection.close()\n",
    "\n",
    "// 접속 성공 후 SQL 테이블을 읽어 DataFrame을 만들 수 있음.\n",
    "val dbDataFrame = sc.read.format(\"jdbc\").option(\"url\", url)\n",
    ".option(\"dbtable\", tablename).option(\"driver\", driver).load()\n",
    "\n",
    "// 조회\n",
    "dbDataFrame.select(\"DEST_COUNTRY_NAME\").distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 9.6.2 쿼리 푸시다운\n",
    "// 스파크는 DataFrame을 만들기 전 데이터베이스 자체에서 데이터를 필터링하도록 만들 수 있음.\n",
    "// 즉, 스파크는 DataFrame에 필터를 명시하면 스파크는 해당 필터에 대한 처리를 데이터베이스로 위임 (push down) 함.\n",
    "// 이는 explain을 통해 확인할 수 있다.\n",
    "dbDataFrame.filter(\"DEST_COUNTRY_NAME\" in ('Auguilla', 'Sweden')).explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 물론 모든 스파크 함수를 데이터베이스에 맞게 변환하지는 못함. \n",
    "// 따라서 경우에 따라 전체 쿼리를 데이터베이스에 전달해 Dataframe을 받아야 하는 경우도 있음.\n",
    "// 처음 DataFrame을 만들 때 테이블명 대신 SQL쿼리를 명시하면 됨.\n",
    "val pushdownQuery = \"\"\"(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info) AS flight_info\"\"\"\n",
    "\n",
    "val dbDataFrame = sc.read.format(\"jdbc\").option(\"url\", url)\n",
    ".option(\"dbtable\", pushdownQuery).option(\"driver\", driver).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**데이터베이스 병렬로 읽기** <br>\n",
    "numPartitions 옵션을 사용해 읽기 및 쓰기용 동시 작업 수를 제한할 수 있는 최대 파티션 수를 설정할 수 있음. <br>\n",
    "이를 통해 과도한 쓰기나 읽기를 막을 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dbDataFrame = sc.read.format(\"jdbc\").option(\"url\", url)\n",
    ".option(\"dbtable\", tab;ename).option(\"driver\", driver)\n",
    ".option(\"numPartitions\", 10).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한, 다른 API 셋에서만 사용할 수 있는 몇 가지 최적화 방법이 있음\n",
    "- 데이터베이스 연결을 통해 명시적으로 조건절을 SQL 데이터베이스에 위임할 수 있다.\n",
    "- 이 방법은 조건절을 명시함으로써 특정 파티션에 특정 데이터의 물리적 위치를 제어할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val props = new java.util.Properties\n",
    "props.serProperty(\"driver\", \"org.splite.JDBC\")\n",
    "val predicates = Array(\n",
    "    \"DEST_COUNTRY_NAME = 'Swedeen' OR ORIGIN_COUNTRY_NAME = 'Sweden'\" ,\n",
    "    \"DEST_COUNTRY_NAME = 'Auguilla' OR ORIGIN_COUNTRY_NAME = 'Auguilla'\"\n",
    ")\n",
    "sc.read.jdbc(url, tablename, predicates, props).show()\n",
    "sc.read.jdbc(url, tablename, predicates, props).rdd.getNumPartitions // 2가 나옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 중복 로우 발생 예제\n",
    "val props = new java.util.Properties\n",
    "props.serProperty(\"driver\", \"org.splite.JDBC\")\n",
    "val predicates = Array(\n",
    "    \"DEST_COUNTRY_NAME != 'Swedeen' OR ORIGIN_COUNTRY_NAME != 'Sweden'\" ,\n",
    "    \"DEST_COUNTRY_NAME != 'Auguilla' OR ORIGIN_COUNTRY_NAME != 'Auguilla'\"\n",
    ")\n",
    "sc.read.jdbc(url, tablename, predicates, props).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**슬라이딩 윈도우 기반 파티셔닝** <br>\n",
    "수치형 count 컬럼 기준으로 분할해볼 것. 처음과 마지막 파티션 사이의 최소값, 최대값을 사용함. <br>\n",
    "이 범위 밖의 모든 값은 첫 번째 또는 마지막 파티션에 속할 것. 이후 파티션의 수를 설정함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val colName = \"count\"\n",
    "val lowerBound = 0L\n",
    "val upperBound = 348113L\n",
    "val numPartitions = 10\n",
    "\n",
    "sc.read.jdbc(url, tablename, colName, lowerBound, upperBound, numPartitions, props)\n",
    ".count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7 텍스트 파일\n",
    "파일의 각 줄은 Dataframe의 레코드가 됨. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7.1 텍스트 파일 읽기\n",
    "textFile 메서드에 텍스트 파일을 지정하기만 하면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                rows|\n",
      "+--------------------+\n",
      "|[DEST_COUNTRY_NAM...|\n",
      "|[United States, R...|\n",
      "|[United States, I...|\n",
      "|[United States, I...|\n",
      "|[Egypt, United St...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.read.textFile(\"../data/flight-data/csv/2010-summary.csv\").selectExpr(\"split(value, ',') as rows\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7.2 텍스트 파일 쓰기\n",
    "텍스트 파일을 쓸대는 문자열 컬럼이 하나만 존재해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.select(\"DEST_COUNTRY_NAME\").write.text(\"../tmp/simple-text-file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.limit(10).select(\"DEST_COUNTRY_NAME\", \"count\")\n",
    ".write.partitionBy(\"count\").text(\"../tmp/five-csv-files2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.8 고급 I/O 개념\n",
    "쓰기 작업 전에 파티션 수를 조절함으로써 병렬로 처리할 파일 수를 제어할 수 있습니다. <br>\n",
    "**버켓팅, 파티셔닝** 을 조절함으로써 데이터의 저장 구조를 제어할 수 있음.\n",
    "\n",
    "#### 9.8.1 분할 가능한 파일 타입과 압축 방식\n",
    "특정 파일 포맷은 기본적으로 분할을 지원하기 때문에, 전체 파일이 아닌 쿼리에 필요한 부분만 읽을 수 있다. (파케이, ORC 포맷)<br>\n",
    "HDFS 시스템을 사용한다면, 여러 블록으로 나누어 저장되기 때문에 훨씬 더 최적화 할 수 있다. <br>\n",
    "모든 압축 방식이 분할을 지원하지 않음. 기본적으로 GZIP 방식을 추천한다.\n",
    "\n",
    "\n",
    "#### 9.8.2 병렬로 데이터 읽기\n",
    "여러 익스큐터가 같은 파일을 동시에 읽지는 않지만, 여러 파일을 동시에 읽을 수 있다. <br> \n",
    "다수의 파일이 존재하는 폴더를 읽을 떄 개별 파일은 DataFrame의 파티션이 됨. 따라서 사용 가능한 익스큐터를 이용해 병렬로 파일을 읽음.\n",
    "\n",
    "#### 9.8.3 병렬로 데이터 쓰기\n",
    "Dataframe이 가진 파티션 수에 따라 달라질 수 있음. 데이터 파티션 당 하나의 파일이 저장됨. <br>\n",
    "아래 예제는 폴더 안에 5개 파일을 생성한다.\n",
    "\n",
    "```csvFile.repartition(5).write.format(\"csv\").save(\"../tmp/multiple.csv\")```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파티셔닝\n",
    "- 어떤 데이터를 어디에 저장할 것인지 제어할 수 있는 기능. 즉, 파티셔닝은 필터링을 자주 사용하는 테이블을 가진 경우에 사용할 수 있는 가장 손쉬운 최적화 방식.\n",
    "- 파티셔닝된 디렉토리 또는 데이터에 파일을 쓸 때 **디렉토리 별로 컬럼 데이터를 인코딩**해 저장함.\n",
    "- 즉, 데이터를 읽을 때는 전체 데이터셋을 스캔하지 않고 필요한 컬럼의 데이터만 읽음.\n",
    "\n",
    "#### 버켓팅\n",
    "- 각 **파일에 저장된 데이터를 제어**할 수 있는 파일 조직화 기법.\n",
    "- 동일한 버킷 ID를 가진 데이터가 하나의 물리적 파티션에 모두 모여 있기 때문에 데이터를 읽을 떄 셔플을 피한다.\n",
    "- 즉, 데이터가 이후에 사용 방식에 맞춰 사전에 파티셔닝이 되므로 나중에 조인, 집계를 위해 파일을 읽을 때 발생하는 고비용의 셔플을 피한다.\n",
    "- ex) PageRank에서 Pair RDD를 미리 버켓팅하면, 같은 id를 가진 노드들은 같은 버켓에 저장됨으로, 이후 조인시 발생하는 고비용의 셔플을 피할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// 파티셔닝\n",
    "csvFile.limit(10).write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\").save(\"../tmp/partitioned-files.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 버켓팅\n",
    "// count 컬럼을 기반으로 버켓팅을 하면 같은 count를 가진 row는 같은 버켓팅에 저장됨.\n",
    "// 보통 버킷 수보다 count 종류의 수가 더 많기 때문에 버킷 당 여러 count들이 저장됨.\n",
    "val numberBuckets = 10\n",
    "val columnToBucketBy = \"count\"\n",
    "\n",
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\n",
    "    .bucketBy(numberBuckets, columnToBucketBy).saveAsTable(\"bucketedFiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.read.format(\"parquet\").load(\"./spark-warehouse/bucketedFiles/part-00000-4d6946cc-ef1f-4eba-811e-85a9dbe86999_00000.c000.snappy.parquet\").sort(\"count\").show(100, false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.read.format(\"parquet\").load(\"./spark-warehouse/bucketedFiles/part-00000-4d6946cc-ef1f-4eba-811e-85a9dbe86999_00009.c000.snappy.parquet\").sort(\"count\").show(100, false)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
