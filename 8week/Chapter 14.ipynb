{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc = org.apache.spark.sql.SparkSession@711ad88c\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.4.3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val sc = SparkSession.builder().getOrCreate()\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myCollection = Array(Spark, The, Definitive, Guide, :, Big, Data, Processing, Made, Simple)\n",
       "words = ParallelCollectionRDD[2] at parallelize at <console>:37\n",
       "supplementalData = Map(Spark -> 1000, Definitive -> 200, Big -> -300, Simple -> 100)\n",
       "suppBroadcast = Broadcast(2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array((Big,-300), (The,0), (Guide,0), (:,0), (Data,0), (Processing,0), (Made,0), (Simple,100), (Definitive,200), (Spark,1000))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "단어나 값의 목록을 가지고 있다고 가정하며, RDD를 생성.\n",
    "*/\n",
    "val myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
    "val words = sc.sparkContext.parallelize(myCollection, 2)\n",
    "\n",
    "/*\n",
    "브로드캐스트 선언.\n",
    "해당 값은 불변성의 값이며, 액션을 실행할 때 클러스터의 모든 노드에 지연 처리 방식으로 복제됨.\n",
    "suppBroadcast의 value 메서드를 사용해 위 예제에서 브로드캐스트된 supplementalData 값을 참조할 수 있음.\n",
    "value 메서드는 직렬화된 함수에서 브로드캐스트된 데이터를 직렬화 하지 않아도 접근할 수 있다. (직렬화와 역직렬화에 대한 부하를 크게 줄임)\n",
    "*/\n",
    "val supplementalData = Map(\"Spark\" -> 1000, \"Definitive\" -> 200, \"Big\" -> -300, \"Simple\" -> 100)\n",
    "val suppBroadcast = sc.sparkContext.broadcast(supplementalData)\n",
    "\n",
    "suppBroadcast.value\n",
    "\n",
    "/*\n",
    "아래 예제는 브래드캐스트된 데이터를 사용해 RDD를 변환하는 것.\n",
    "해당 key가 있다면, 해당 key의 value를 아니면 0으로 채움.\n",
    "*/\n",
    "words.map(word => (word, suppBroadcast.value.getOrElse(word, 0)))\n",
    "    .sortBy(wordPair => wordPair._2)\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "해당 예제는 Dataset을 이용함.\n",
    "*/\n",
    "import sc.implicits._\n",
    "case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt)\n",
    "val flights = sc.read.parquet(\"../data/flight-data/parquet/2010-summary.parquet\").as[Flight]\n",
    "\n",
    "/*\n",
    "출발지나 도착지가 중국인 항공편 수를 구하는 어큐물레이터.\n",
    "아래 예제는 이름이 지정되지 않은 어큐물레이터를 생성함.\n",
    "*/\n",
    "import org.apache.spark.util.LongAccumulator\n",
    "val accUnnamed = new LongAccumulator\n",
    "val acc = sc.sparkContext.register(accUnnamed)\n",
    "\n",
    "/*\n",
    "아래 예제는 이름이 지정된 어큐물레이터를 생성함.\n",
    "*/\n",
    "val accChina = new LongAccumulator\n",
    "val accChina2 = sc.sparkContext.longAccumulator(\"China\")\n",
    "// OR\n",
    "sc.sparkContext.register(accChina, \"China\")\n",
    "\n",
    "def accChinaFunc(flight_row: Flight) = {\n",
    "    val destination = flight_row.DEST_COUNTRY_NAME\n",
    "    val origin = flight_row.ORIGIN_COUNTRY_NAME\n",
    "    \n",
    "    if(destination == \"China\") {\n",
    "        accChina.add(flight_row.count.toLong)\n",
    "    }\n",
    "    if (origin == \"China\") {\n",
    "        accChina.add(flight_row.count.toLong)\n",
    "    }\n",
    "}\n",
    "\n",
    "flights.foreach(flight_row => accChinaFunc(flight_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "어큐물레이터 직접 정의할 수 있다. (사용자 정의 어큐물레이터))\n",
    "*/\n",
    "\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import org.apache.spark.util.AccumulatorV2\n",
    "\n",
    "val arr = ArrayBuffer[BigInt]()\n",
    "\n",
    "class EvenAccumulator extends AccumulatorV2[BigInt, BigInt] {\n",
    "    private var num:BigInt = 0\n",
    "    def reset(): Unit = {\n",
    "        this.num = 0\n",
    "    }\n",
    "    def add(intValue: BigInt): Unit = {\n",
    "        if (intValue % 2 == 0) {\n",
    "            this.num += intValue\n",
    "        }\n",
    "    }\n",
    "    def merge(other: AccumulatorV2[BigInt, BigInt]): Unit = {\n",
    "        this.num += other.value\n",
    "    }\n",
    "    def value():BigInt = {\n",
    "        this.num\n",
    "    }\n",
    "    def copy(): AccumatorV2[BigInt, BigInt] = {\n",
    "        new EvenAccumulator\n",
    "    }\n",
    "    def isZero():Boolean = {\n",
    "        this.num == 0\n",
    "    }\n",
    "}\n",
    "\n",
    "val acc = new EvenAccumulator\n",
    "val newAcc = sc.register(acc, \"evenAcc\")\n",
    "\n",
    "acc.value\n",
    "flights.foreach(flight_row => acc.add(flight_row.count))\n",
    "acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
